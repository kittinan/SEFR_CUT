{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# encoding: utf-8\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, \\\n",
    "    Concatenate, Flatten, SpatialDropout1D, \\\n",
    "    BatchNormalization, Conv1D, Maximum, ZeroPadding1D\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def conv_unit(inp, n_gram, no_word=200, window=2):\n",
    "    out = Conv1D(no_word, window, strides=1, padding=\"valid\", activation='relu')(inp)\n",
    "    out = TimeDistributed(Dense(5, input_shape=(n_gram, no_word)))(out)\n",
    "    out = ZeroPadding1D(padding=(0, window - 1))(out)\n",
    "    return out\n",
    "\n",
    "def get_convo_nn2(no_word=200, n_gram=21, no_char=178):\n",
    "    input1 = Input(shape=(n_gram,))\n",
    "    input2 = Input(shape=(n_gram,))\n",
    "\n",
    "    a = Embedding(no_char, 32, input_length=n_gram)(input1)\n",
    "    a = SpatialDropout1D(0.15)(a)\n",
    "    a = BatchNormalization()(a)\n",
    "\n",
    "    a_concat = []\n",
    "    for i in range(1,9):\n",
    "        a_concat.append(conv_unit(a, n_gram, no_word, window=i))\n",
    "    for i in range(9,12):\n",
    "        a_concat.append(conv_unit(a, n_gram, no_word - 50, window=i))\n",
    "    a_concat.append(conv_unit(a, n_gram, no_word - 100, window=12))\n",
    "    a_sum = Maximum()(a_concat)\n",
    "\n",
    "    b = Embedding(12, 12, input_length=n_gram)(input2)\n",
    "    b = SpatialDropout1D(0.15)(b)\n",
    "\n",
    "    x = Concatenate(axis=-1)([a, a_sum, b])\n",
    "    #x = Concatenate(axis=-1)([a_sum, b])\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    out = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs=[input1, input2], outputs=out)\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = get_convo_nn2()\n",
    "#model.load_weights('weight/model_weight.h5')  #For WS\n",
    "model.load_weights('weight/model_weight_tnhc.h5') #For TNHC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from preprocessing import preprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy as cp\n",
    "from pythainlp.benchmarks.word_tokenization import benchmark\n",
    "prepro = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_list = ['wisesight-testset']\n",
    "#test_list = ['testset']\n",
    "test_list = ['tnhc_test']\n",
    "\n",
    "x,y_true = prepro.preprocess_x_y(test_list)\n",
    "y_true = [j for sub in y_true for j in sub if len(j) > 1]\n",
    "x = [j for sub in x for j in sub if len(j) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for idx,item in enumerate(x):\n",
    "    char_,type_ = prepro.create_feature_array(item)\n",
    "    ans = model.predict([char_, type_])\n",
    "    y_pred.append(ans)\n",
    "    \n",
    "y_pred_ = prepro.preprocessing_y_pred(y_pred)\n",
    "y_pred = list(map(prepro.argmax_function,y_pred_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_data = [j for sub in y_pred for j in sub]\n",
    "y_true_data = [j for sub in y_true for j in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "def eval_function(y_true,y_pred):\n",
    "    f1_score_entropy=[]; \n",
    "    for index,_ in enumerate(y_pred):\n",
    "        precision, recall, fscore, _ = precision_recall_fscore_support(y_true[index], y_pred[index], average='binary')\n",
    "        #print(f'Precision:{precision}, Recall:{recall}, F1:{fscore}')\n",
    "        f1_score_entropy.append(fscore)\n",
    "    return np.mean(f1_score_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9624969158647916"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_function([y_true_data],[y_test_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(y_pred_boolean,x_data):\n",
    "    x_ = cp.deepcopy(x_data)\n",
    "    answer = []\n",
    "    for idx,items in enumerate(y_pred_boolean):\n",
    "        text = \"\"\n",
    "        for index,item in enumerate(items):\n",
    "            if(item == 1):\n",
    "                text +='|'\n",
    "            text +=x_[idx][index]\n",
    "        answer.append(text)\n",
    "    return answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 0.8792367221630863\n"
     ]
    }
   ],
   "source": [
    "y_true_data = [j for sub in y_true for j in sub]\n",
    "y_original_data = [j for sub in y_pred for j in sub]\n",
    "\n",
    "x_data = ''\n",
    "for item in x:\n",
    "    x_data+=item\n",
    "    \n",
    "dc_pred = [y_original_data]\n",
    "dc_pred = cut(dc_pred,[x_data])\n",
    "true_pred = [y_true_data]\n",
    "true_pred = cut(true_pred,[x_data])\n",
    "\n",
    "from itertools import accumulate\n",
    "import operator\n",
    "\n",
    "def evaluate(train : list, test: list) -> tuple:\n",
    "    train_acc = list(accumulate(map(len, train), func = operator.add))\n",
    "    test_acc = list(accumulate(map(len, test), func = operator.add))\n",
    "    train_set = set(zip([0,*train_acc], train_acc))\n",
    "    test_set = set(zip([0,*test_acc], test_acc))\n",
    "    correct = len(train_set & test_set)\n",
    "    pre = correct/len(test)\n",
    "    re = correct/len(test)\n",
    "    f1 = (2*pre*re)/(pre+re)\n",
    "    return f1\n",
    "\n",
    "dc_list = dc_pred[0].split('|')\n",
    "true_list = true_pred[0].split('|')\n",
    "print('Baseline:',evaluate(true_list,dc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2word(l):\n",
    "\tdata = []\n",
    "\ttemp = \"\"\n",
    "\tfor i,c in enumerate(l):\n",
    "\t\tif (c == 1 and i>0):\n",
    "\t\t\tdata.append(temp)\n",
    "\t\t\ttemp = \"\"\n",
    "\t\ttemp += str(c)\n",
    "\t\tif i==len(l)-1:\n",
    "\t\t\tdata.append(temp)\n",
    "\treturn data\n",
    "\n",
    "def correct2(y_true,y_pred):\n",
    "\tl_true = list2word(y_true)\n",
    "\tl_pred = list2word(y_pred)\n",
    "\t#print(l_true)\n",
    "\t#print(l_pred)\n",
    "\tcorrect = []\n",
    "\tchar_index = 0\n",
    "\tfor i,w in enumerate(l_pred):\n",
    "\t\t#print(w)\n",
    "\t\tif w in l_true:\n",
    "\t\t\tif (y_true[char_index]+y_pred[char_index]==2 and y_true[char_index:char_index+len(w)-1]==y_pred[char_index:char_index+len(w)-1]): # ถ้า index char เป็น 1 ทั้งค่าจริงและทำนาย และ ถ้า list word นั้นเหมือนกันทั้งค่าจริงและทำนาย\n",
    "\t\t\t\tcorrect.append(w)\n",
    "\t\tchar_index+=len(w)\n",
    "\tnum_correct = len(correct)\n",
    "\tprecision = num_correct/len(l_pred)\n",
    "\trecall = num_correct/len(l_true)\n",
    "\t#print(precision,recall,correct)\n",
    "\ttry:\n",
    "\t\tf1 = 2*precision*recall / (precision + recall)\n",
    "\texcept ZeroDivisionError:\n",
    "\t\tf1=0.0\n",
    "\treturn {\"precision\":precision,\"recall\":recall,\"f1\":f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'precision': 0.9228828117936199,\n",
       " 'recall': 0.9371489145041608,\n",
       " 'f1': 0.9299611539512809}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct2(y_true_data,y_original_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
